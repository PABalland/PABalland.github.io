Retrieved authors from all Huggingface models, we took the top 500 with largest sum of likes.
We classified from their HF profiles (or other websites if needed) whether they belong to an org or person, and found the link to their crunchbase profile or in case that is missing, noted their headquarter location from another trustworthy website, for raw data see here: https://docs.google.com/spreadsheets/d/1Laicv-x7fjCJMyEgYbWuaXh8dIENcLhy-BF9TasxKRM/edit?usp=sharing

From non-crunchbase websites we took the headquarters locations unless this is very ambiguous (only happened in the case of Pruna AI and split fractional between Germany and France there)
From HQ locations we took the countries (See map_country_raw.csv, here: https://www.dropbox.com/scl/fi/4mmzjedswif4dkyodvk65/map_country_raw.csv?rlkey=4xppw2infz48xsws29oszu0fa&st=u9uwkd0a&dl=0)

Per country counted the amount of orgs in the top 500 likes with an identified location, and summed likes, downloads (last 30 days) and downloads of all time and connected ISO-2 country codes to the country names. Spot checks were done on ~10 countries by convenience sampling (countries that stood out or are relevant for analysis, Vietnam, Bangladesh, Canada, France etc.) and manual summing was done for them. Overall discrepancies between likes and downloads were scanned only high-level.