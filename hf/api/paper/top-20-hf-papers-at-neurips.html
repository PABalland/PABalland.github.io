<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Top 20 AI Research Papers 2024</title>
    <style>
    body {
        font-family: system-ui, -apple-system, sans-serif;
        background: linear-gradient(135deg, #f6f8fd 0%, #f0f4f8 100%);
        padding: 12px;
        margin: 0;
        height: 100vh;
        overflow: hidden;
    }
    .papers-container {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        gap: 8px;
        max-width: 1600px;
        margin: 0 auto;
        height: calc(100vh - 60px);
    }
    .category-box {
        padding: 12px;
        border-radius: 12px;
        display: flex;
        flex-direction: column;
    }
    .category-box.reasoning {
        background: rgba(255, 210, 30, 0.15); /* #FFD21E */
    }
    .category-box.vision {
        background: rgba(255, 157, 0, 0.15); /* #FF9D00 */
    }
    .category-box.generative {
        background: rgba(38, 86, 210, 0.15); /* #2656D2 */
    }
    .category-box.architecture {
        background: rgba(121, 29, 129, 0.15); /* #791D81 */
    }
    .papers-grid {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        gap: 4px;
        flex-grow: 1;
    }
    .category-title {
        font-size: 1.1em;
        margin-bottom: 10px;
        font-weight: bold;
        color: #333;
    }
    .paper-box {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 8px;
        padding: 10px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        position: relative;
        height: 80px;
        overflow: hidden;
        transition: all 0.3s ease;
    }
    .paper-box:hover {
        transform: translateY(-4px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        background: rgba(255, 255, 255, 1);
    }
    .votes {
        position: absolute;
        top: 6px;
        right: 6px;
        background: linear-gradient(135deg, #2656D2, #791D81); 
        color: white;
        padding: 2px 6px;
        border-radius: 8px;
        font-size: 0.75em;
        font-weight: 600;
    }
    .ranking {
        position: absolute;
        top: 6px;
        left: 6px;
        background: #FFD21E; 
        color: white;
        padding: 2px 6px;
        border-radius: 8px;
        font-size: 0.75em;
        font-weight: bold;
    }
    .title {
        font-weight: 600;
        font-size: 1em;
        margin: 2px 0;
        color: #1a1a1a;
        line-height: 1.2;
        padding: 0 10px;
        margin-top: 20px;
    }
    .authors {
        font-size: 0.85em;
        color: #666;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
        padding: 0 8px;
    }
    .summary {
        display: none;
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(255, 255, 255, 0.9);
        color: #333;
        padding: 10px;
        border-radius: 8px;
        z-index: 1;
        overflow: auto;
        font-size: 0.75em;
        line-height: 1.2;
    }
    .paper-box:hover .summary {
        display: block;
    }
    </style>
</head>
<body>
    <div class="papers-container">
    <!-- Language Model Reasoning -->
    <div class="category-box reasoning">
    <div class="category-title">üß† Language Model Reasoning</div>
    <div class="papers-grid">
    <div class="paper-box">
    <div class="ranking">#1</div>
    <div class="votes">109 votes</div>
    <div class="title">Self-Discover: LLMs Self-Compose</div>
    <div class="authors">Pei Zhou, et al.</div>
    <div class="summary">We introduce SELF-DISCOVER, a general framework for LLMs to self-discover task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods...</div>
    </div>
    <div class="paper-box">
    <div class="ranking">#2</div>
    <div class="votes">101 votes</div>
    <div class="title">Chain-of-Thought Without Prompting</div>
    <div class="authors">Xuezhi Wang, Denny Zhou</div>
    <div class="summary">In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting...</div>
    </div>
    <div class="paper-box">
    <div class="votes">91 votes</div>
    <div class="title">ReFT: Representation Finetuning</div>
    <div class="authors">Zhengxuan Wu, et al.</div>
    <div class="summary">Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information...</div>
    </div>
    <div class="paper-box">
    <div class="votes">53 votes</div>
    <div class="title">Self-Improvement of LLMs</div>
    <div class="authors">Ye Tian, et al.</div>
    <div class="summary">Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involve complex reasoning and planning...</div>
    </div>
    <div class="paper-box">
    <div class="votes">52 votes</div>
    <div class="title">Make Your LLM Fully Utilize Context</div>
    <div class="authors">Shengnan An, et al.</div>
    <div class="summary">While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge...</div>
    </div>
    <div class="paper-box">
    <div class="votes">52 votes</div>
    <div class="title">Scaling Laws with Vocabulary</div>
    <div class="authors">Chaofan Tao, et al.</div>
    <div class="summary">Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size...</div>
    </div>
    </div>
    </div>

    <!-- Vision & Multimodal -->
    <div class="category-box vision">
    <div class="category-title">üëÅÔ∏è Vision-Language Models</div>
    <div class="papers-grid">
    <div class="paper-box">
    <div class="ranking">#3</div>
    <div class="votes">100 votes</div>
    <div class="title">What matters in vision-language models?</div>
    <div class="authors">Hugo Lauren√ßon, et al.</div>
    <div class="summary">The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers...</div>
    </div>
    <div class="paper-box">
    <div class="votes">72 votes</div>
    <div class="title">ShareGPT4Video</div>
    <div class="authors">Lin Chen, et al.</div>
    <div class="summary">We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions...</div>
    </div>
    <div class="paper-box">
    <div class="votes">58 votes</div>
    <div class="title">Cambrian-1: Vision-Centric MLLMs</div>
    <div class="authors">Shengbang Tong, et al.</div>
    <div class="summary">We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach...</div>
    </div>
    <div class="paper-box">
    <div class="votes">52 votes</div>
    <div class="title">OMG-LLaVA</div>
    <div class="authors">Tao Zhang, et al.</div>
    <div class="summary">Current universal segmentation methods demonstrate strong capabilities in pixel-level image and video understanding. However, they lack reasoning abilities and cannot be controlled via text instructions...</div>
    </div>
    <div class="paper-box">
    <div class="votes">52 votes</div>
    <div class="title">Needle In A Multimodal Haystack</div>
    <div class="authors">Weiyun Wang, et al.</div>
    <div class="summary">With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive...</div>
    </div>
    <div class="paper-box">
    <div class="votes">61 votes</div>
    <div class="title">MMDU: Multi-Image Dialog</div>
    <div class="authors">Ziyu Liu, et al.</div>
    <div class="summary">Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs)...</div>
    </div>
    </div>
    </div>

    <!-- Generative Models -->
    <div class="category-box generative">
    <div class="category-title">üé® Generative Models</div>
    <div class="papers-grid">
    <div class="paper-box">
    <div class="votes">93 votes</div>
    <div class="title">Depth Anything V2</div>
    <div class="authors">Lihe Yang, et al.</div>
    <div class="summary">This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model...</div>
    </div>
    <div class="paper-box">
    <div class="votes">64 votes</div>
    <div class="title">Visual AutoRegressive Modeling</div>
    <div class="authors">Keyu Tian, et al.</div>
    <div class="summary">We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction"...</div>
    </div>
    <div class="paper-box">
    <div class="votes">55 votes</div>
    <div class="title">An Image is Worth 32 Tokens</div>
    <div class="authors">Qihang Yu, et al.</div>
    <div class="summary">Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images...</div>
    </div>
    <div class="paper-box">
    <div class="votes">53 votes</div>
    <div class="title">FIFO-Diffusion</div>
    <div class="authors">Jihwan Kim, et al.</div>
    <div class="summary">We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation...</div>
    </div>
    </div>
    </div>

    <!-- Model Architecture -->
    <div class="category-box architecture">
    <div class="category-title">‚öôÔ∏è Model Architecture</div>
    <div class="papers-grid">
    <div class="paper-box">
    <div class="votes">63 votes</div>
    <div class="title">Megalodon: Efficient LLM Pretraining</div>
    <div class="authors">Xuezhe Ma, et al.</div>
    <div class="summary">The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences...</div>
    </div>
    <div class="paper-box">
    <div class="votes">62 votes</div>
    <div class="title">SaulLM: Domain Adaptation</div>
    <div class="authors">Pierre Colombo, et al.</div>
    <div class="summary">In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector...</div>
    </div>
    <div class="paper-box">
    <div class="votes">59 votes</div>
    <div class="title">Multi-Head Mixture-of-Experts</div>
    <div class="authors">Xun Wu, et al.</div>
    <div class="summary">Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs...</div>
    </div>
    <div class="paper-box">
    <div class="votes">53 votes</div>
    <div class="title">Meteor: Mamba-based Traversal</div>
    <div class="authors">Byung-Kwan Lee, et al.</div>
    <div class="summary">The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning...</div>
    </div>
    </div>
    </div>
    </div>
</body>
</html>