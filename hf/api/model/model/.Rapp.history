100*0.25
100/0.25
100*0.25
(100*0.75)*0.7
52.5/0.7
(52.5/0.7)/0.25
(52.5/0.7)/0.75
142*0.75
106.5*0.7
190000*0.8
15000*24
5000*24
10000*24
200000/24
12883+3805
16688-10515
540*1000
540/24
22.5*1000
150*5
5985.6*0.35
5985.6*0.35+5985.6*0.65
5985.6*0.6
5985.6*0.6+2094.6
5985.6*0.65
3890.64+2094.96
3890.64 +2094.96
4+8+10+4+10+5+5
540*100
71000/12
54000/12
71964/12
72.5-35
37.5*2
71.96/12
900/1600
3*4
150/22
9*12
42/10
130*0.7
100/0.7
142*0.7
12000*0.7
9000*0.7
86 + 19
5*30
0.6*71000
440/1120
839/2300
37+21
11000*0.35
15/52
5/52
20+35+60+60
15+35+60+60
9+5
103+14
8400+5000+6300
26400*0.8
72693/2*(0.3)
27500-21200
36347-6252
35/120
120/35
3.428571*25
3.428571*15
3.428571*10
3400/5
1904/680
toupper("Mapping the Competitiveness of Sweden in Key Strategic Technologies")
8400/12
700*9
700*8
21*700
40000*0.7
40000/700
37337-33000
60000/0.7
60000/0.3
200000*0.3
40000/650
200/12
16.6*0.3
1100*0.7
770.00 *8
770.00 *11
770.00 *11.5
770.00 *11
11 *	770.00
57/(143+112+30+49+57+60)
(143+112+30+49+57+60)
4+3.5+3.5
11/15
0.73*20
13450.00 + 63000.00
184*735.57
184*735.57/8
tolower("MAPPING THE USE OF ARTIFICIAL INTELLIGENCE IN#
THE SUSTAINABLE DEVELOPMENT GOALS")
4*5*8
x=830+290*2+720+50
x
x*502
x/502
x*1000/502
259.94*0.79
983.6
983.6*0.79
766.63/983.6
66.57/193.37
66.57/317.00
295.2/360
360*0.18
140000/366
5464.78 + 1500 - 1951.26 - 604.93
350000*0.7
n=10
(n*(n-1))/2
n=5
(n*(n-1))/2
n=5000
(n*(n-1))/2
(n*(n-1))/2/6
((n*(n-1))/2)/6
((n*(n-1))/2)/1000000
0.07615192-0.00316473+0.00952962-0.00039603
0.09-0.08212078
0.041+0.041
2323.42+2352
4675.42 + 1500 - 1951.26 - 604.93
800/3619.23
6300/700
2000*2*5+200*4*5+3000*5
512/2
724.92+732.81+406.27+804.6
525*4
40*8
40*800
30*800
40237-36800.00
6500+5000+9000+5000
33621-25500
40237-39800.00
13-6
8008.76-4900
160/491
80/491
2/14
4248537+964775
3589465/5200000
(5200000-3589465)/5200000
7.7*0.309
7.7*0.69
20747470/2
3589269/5286939
1 - 3589269/5286939
7765944*0.6788936
450*(-0.7-0.5+6.8)
12207.7*0.21
12207.7/2
6103.85*0.21
26400.00*1.5
5500*12
library(httr)library(jsonlite)apiKey <- "sk-proj-gujYYeDstywkdUx6JtZxT3BlbkFJp5ICEDp4xaXfnbFRx2hO"df = fromJSON("https://www.paballand.com/domain/industry.json")cpc = NULL i = "Oil & Gas Drilling"for (i in unique (df$sub_industry_name)){prompt <- paste0("give me a list of cpc relevant to", i, "and their description.Dont tell me what cpc is,                  just bullet points, a give cpc (code): description")response <- POST(  url = "https://api.openai.com/v1/chat/completions",   add_headers(Authorization = paste("Bearer", apiKey)),  content_type_json(),  encode = "json",  body = list(    model = "gpt-4",    temperature = 1,    messages = list(list(      role = "user",       content = prompt    ))  ))content(response)$choices[[1]]$message$contentlines <- unlist(strsplit(content(response)$choices[[1]]$message$content, "\n"))# Initialize empty vectors for CPC codes and descriptionscpc_codes <- c()descriptions <- c()# Loop through each line and extract the CPC code and descr
iptionfor (line in lines) {  # Use regular expressions to extract the CPC code and description  cpc_code <- sub("^[0-9]+\\. ([A-Z0-9/]+):.*$", "\\1", line)  description <- sub("^[0-9]+\\. [A-Z0-9/]+: (.*)$", "\\1", line)#
  # Append to the vectors  cpc_codes <- c(cpc_codes, cpc_code)  descriptions <- c(descriptions, description)}# Create a data framecpc_table <- data.frame(CPC_Code = cpc_codes, Description = descriptions, stringsAsFactors = FALSE)cpc_table <- strsplit(cpc_table$CPC_Code, ":")cpc_table = do.call(rbind, lapply(cpc_table, function(x) data.frame(Col1 = x[1], Col2 = x[2])))cpc_table$sub_industry_name = i cpc_table$cpc_code <- str_extract(cpc_table$Col1, "(?<=\\.).*")cpc_table$cpc_description <- cpc_table$Col2cpc_table = cpc_table[, c("sub_industry_name", "cpc_code", "cpc_description")]cpc_table = cpc_table[complete.cases(cpc_table),]cpc = rbind (cpc, cpc_table)}
library(httr)library(jsonlite)library(stringr)apiKey <- "sk-proj-gujYYeDstywkdUx6JtZxT3BlbkFJp5ICEDp4xaXfnbFRx2hO"df = fromJSON("https://www.paballand.com/domain/industry.json")cpc = NULL i = "Oil & Gas Drilling"for (i in unique (df$sub_industry_name)){prompt <- paste0("give me a list of cpc relevant to", i, "and their description.Dont tell me what cpc is,                  just bullet points, a give cpc (code): description")response <- POST(  url = "https://api.openai.com/v1/chat/completions",   add_headers(Authorization = paste("Bearer", apiKey)),  content_type_json(),  encode = "json",  body = list(    model = "gpt-4",    temperature = 1,    messages = list(list(      role = "user",       content = prompt    ))  ))content(response)$choices[[1]]$message$contentlines <- unlist(strsplit(content(response)$choices[[1]]$message$content, "\n"))# Initialize empty vectors for CPC codes and descriptionscpc_codes <- c()descriptions <- c()# Loop through each line and extract the C
PC code and descriptionfor (line in lines) {  # Use regular expressions to extract the CPC code and description  cpc_code <- sub("^[0-9]+\\. ([A-Z0-9/]+):.*$", "\\1", line)  description <- sub("^[0-9]+\\. [A-Z0-9/]+: (.*)$", "\\1", line)#
  # Append to the vectors  cpc_codes <- c(cpc_codes, cpc_code)  descriptions <- c(descriptions, description)}# Create a data framecpc_table <- data.frame(CPC_Code = cpc_codes, Description = descriptions, stringsAsFactors = FALSE)cpc_table <- strsplit(cpc_table$CPC_Code, ":")cpc_table = do.call(rbind, lapply(cpc_table, function(x) data.frame(Col1 = x[1], Col2 = x[2])))cpc_table$sub_industry_name = i cpc_table$cpc_code <- str_extract(cpc_table$Col1, "(?<=\\.).*")cpc_table$cpc_description <- cpc_table$Col2cpc_table = cpc_table[, c("sub_industry_name", "cpc_code", "cpc_description")]cpc_table = cpc_table[complete.cases(cpc_table),]cpc = rbind (cpc, cpc_table)}
1+1
head(cpc)
getwd()
write.csv(cpc, "industry-cpc.csv", row.names = F)
tail(cpc)
3440/2
1437/3440
3440/1430
df1 <- data.frame(A = c(1, 2), B = c(3, 4))#
rownames(df1) <- c("kk", "kk")#
#
df2 <- data.frame(A = c(5, 6), B = c(7, 8))#
rownames(df2) <- c("kk", "kk")#
#
# Remove row names#
rownames(df1) <- NULL#
rownames(df2) <- NULL#
#
# Combine using rbind#
combined_df <- rbind(df1, df2)#
print(combined_df)
df1
df2
rownmaes(df2)
rownames(df2)
rownames(df1)
4948.9/7
getOption("stringsAsFactors")
9*500*12
577/2
142#
89#
8#
2#
0
550*0.75
0.5*250
0.5*250*30
11-8
20809+5000+14700+20809+20809+20809
40000-33621
60/5
12*0.7
10000-1700
8300*0.8
65*(8/12)
65*(12/8)
6813.00*0.9
400*0.2
150+890.31+280
(29+27)
56/732
997.30/2
library(EconGeo)
?rca
26400*1.5
39600*0.8
39600.00*6/8
39600.00*0.8
39600.00*0.2
install.packages("languageserver")
---#
---#
#
<link rel="stylesheet" href="styles.css" type="text/css">#
#
<img src="images/pierre-alexandre-balland-full.jpg" style="width:40%; border:0px solid; margin-right: 30px ; margin-top: 20px; margin-bottom: 10px" align="left">#
#
<style>#
body {#
text-align: justify; font-size: 18px;}#
</style>#
#
<br>#
Pierre-Alexandre Balland currently serves as the Chief Data Scientist of the [Centre for European Policy Studies](https://www.ceps.eu/ceps-staff/pierre-alexandre-balland/) and is a Visiting Professor of the Growth Lab at [Harvard University](https://growthlab.hks.harvard.edu/people/pierre-alexandre-balland). #
#
He is a [French economist](https://scholar.google.com/citations?user=ORZg_McAAAAJ&hl=en&oi=ao) and one of Europe's leading experts on complex economic systems, artificial intelligence, and blockchain technologies. #
#
<br>#
At the Centre for European Policy Studies (CEPS), Pierre-Alex leads a team that leverages artificial intelligence and data science tools to address a wide range of public policy challenges. At the Harvard Kennedy School's Growth Lab, he works with the academic team to advance fundamental research on economic complexity and its applications to technological change, industrial policy, green growth, and the future of work.#
#
He previously held positions at [Utrecht University](https://www.uu.nl/staff/PMABalland/Profile), [MIT](https://www.media.mit.edu/people/balland/overview/) and  [UCLA](https://www.ucla.edu/). He is a Visiting Professor at the [Artificial and Natural Intelligence Toulouse Institute](https://en.univ-toulouse.fr/aniti) and a research fellow at the [Center for Complex Systems Studies](https://www.uu.nl/en/research/centre-for-complex-systems-studies-ccss). Pierre-Alex is also a high-level policy expert that  currently serves in the [ESIR](https://ec.europa.eu/info/research-and-innovation/strategy/support-policy-making/support-eu-research-and-innovation-policy-making/esir_en) group to advise the European Commission on innovation policy. #
#
[Download Full CV](https://www.paballand.com/cv.pdf) | pa@argosgroup.net
#great principles
#reg burden
#specific examples
#int comparison
165/751
73+62
187/720
135/751
170/751
6890.8*0.2
90*3
270/4
67.5/90
270/5
54/90
8*2.5
600 + 550 + 80 + 100 + 100 + 200 + 400 + 100 + 800 + 500 + 100 + 200 + 300 + 200 + 150 + 1400
12000-5780
1210-500
nov-dec-jan-feb-mar-abril =
12*7
950*0.7
3*2*4
56-22-20-14-4
56-22-20-14
56-22-16-14-4
22+16+14+4
16*8
8000*0.2
d3plus.network = function (links) {#
  colnames (links) = c("from", "to", "weight")#
  if (!exists("nodes")) nodes <- data.frame(id = unique (links$from))#
  if (!exists("savename")) savename <- "network"#
  if (!exists("topedges")) topedges <- 2#
  if (!exists("topedges2")) topedges2 <- 4#
  if (!exists("spread_factor")) spread_factor <- 4#
  if (!exists("spread_angle")) spread_angle <- 45#
  library(data.table)#
  library (EconGeo)#
  library (jsonlite)#
  library(igraph)#
  source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
  source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/spread.fr.algo.R")#
  source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/layout.networkd3.R")#
  # html source location#
  setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/_source")#
  if (!exists("p1")) p1 <- paste(readLines("graph-part-1.html"), collapse="\n") #before json data#
  if (!exists("p3")) p3 <- paste(readLines("graph-part-3.html"), collapse="\n") #before json data#
  if (!exists("p5")) p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
  g <- graph_from_data_frame(links, directed=F, vertices=nodes)#
  #g <- graph_from_data_frame(links, directed=F)#
  layout_coords_full <- layout.networkd3(g)#
  E(g)$weight <- -links$weight # Negate the weights to find the maximum spanning tree#
  mst <- mst(g, weights = E(g)$weight) # Compute the maximum spanning tree#
  E(mst)$weight <- abs(E(mst)$weight) # Restore the original weights (positive)#
  # now add top edges#
  df3 = links[order(-links$weight),]#
  df3 = df3[1:(length(unique(df3$from))*topedges),]#
  g <- graph_from_data_frame(df3, directed=F, vertices=nodes)#
  #combined_graph <- mst#
  new_edges <- get.edges(g, E(g))#
  mst <- add_edges(mst, edges = new_edges, attr = edge.attributes(g))#
  # add top links per node now (not before not to mess up the layout)#
  #df3 = keep.top.links(links, topedges2)#
  #df3 = df3[complete.cases(df3),]#
  #g <- graph_from_data_frame(df3, directed=F, vertices=nodes)#
  #new_edges <- get.edges(g, E(g))#
  #mst <- add_edges(mst, edges = new_edges, attr = edge.attributes(g))#
  # Usage:#
  layout_coords <- layout.networkd3(mst)#
  layout_coords = spread.fr.algo(layout_coords,factor = spread_factor, angle = spread_angle)#
#
  # compute x/y#
  layout_fr <- spread.fr.algo(layout_with_fr(mst), factor = spread_factor, angle = spread_angle) # spread Fruchterman-Reingold layout#
  V(mst)$x_fr <- layout_fr[,1] # Fruchterman-Reingold layout#
  V(mst)$y_fr <- layout_fr[,2] # Fruchterman-Reingold layout#
  V(mst)$x_kk <- layout_with_kk(mst)[,1] # Kamada-Kawai layout#
  V(mst)$y_kk <- layout_with_kk(mst)[,2] # Kamada-Kawai layout#
  V(mst)$x_d3 <- layout_coords[,1] # Fruchterman-Reingold layout#
  V(mst)$y_d3 <- layout_coords[,2] # Fruchterman-Reingold layout#
  V(mst)$x_d3_full <- layout_coords_full[,1] # Fruchterman-Reingold layout#
  V(mst)$y_d3_full <- layout_coords_full[,2] # Fruchterman-Reingold layout#
  # detect communities for colors #
  V(mst)$parent_louvain = as.factor(membership(cluster_louvain(mst)))#
  V(mst)$color_louvain <- rainbow(length(unique(membership(cluster_louvain(mst)))))[membership(cluster_louvain(mst))]#
  # size in the mst network#
  V(mst)$size_g = closeness(mst)#
  # replace possible missing node variables#
  if (is.null(vertex_attr(mst, "size"))) {V(mst)$size <- V(mst)$size_g}#
  if (is.null(vertex_attr(mst, "parent"))) {V(mst)$parent <- V(mst)$parent_louvain}#
  if (is.null(vertex_attr(mst, "color"))) {V(mst)$color <- V(mst)$color_louvain}#
  if (is.null(vertex_attr(mst, "x"))) {V(mst)$x <- V(mst)$x_d3}#
  if (is.null(vertex_attr(mst, "y"))) {V(mst)$y <- V(mst)$y_d3}#
  if (is.null(vertex_attr(mst, "related"))) {V(mst)$related <- "No information"}#
  g = mst#
  # format to d3plus#
  V(g)$name2 = V(g)$name #
  V(g)$index = c(1:length(V(g)$name))#
  V(g)$name = as.numeric(factor (V(g)$index))-1 # id & links needs to start at 0 in D3plus#
  #vertex_attr(g) # see all attributes#
  nodes2 = data.frame(#
    id = V(g)$name2, #
    id2 = V(g)$name, #
    x = V(g)$x,#
    y = V(g)$y, #
    color = V(g)$color, #
    parent = V(g)$parent, #
    size = V(g)$size, #
    related = V(g)$related#
  )#
  links2 <- as.data.frame(cbind( as_edgelist(g) , round( E(g)$weight, 3 )))#
  colnames (links2) = c("source", "target", "weight")#
  links2 = subset (links2, links2$weight>0)#
  # convert to JSON#
  p2 = toJSON(nodes2, encoding = "latin-ascii")#
  p2 = gsub('"x"', 'x', p2) # remove quotes that cause a pb#
  p2 = gsub('"y"', 'y', p2) # remove quotes that cause a pb#
  #p5 = gsub(".sizeMax(40)\n", "hola", p5) # remove quotes that cause a pb#
  p4 = toJSON(links2)#
  p4 = gsub('"', '', p4) # remove quotes that cause a pb#
  all = paste (p1, p2, p3, p4, p5, collapse="\n")#
  # save as d3plus network#
  setwd(dir)#
  writeLines(all, paste0(savename, ".html"))#
  }
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 2#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 3#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 3#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 3#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 3#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 3#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
####
### NETWORK GRAPH OF MODELS#
####
#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
#p1 <- paste(readLines("part-1.html"), collapse="\n") #before json data#
#p3 <- paste(readLines("part-3.html"), collapse="\n") #before json data#
#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/sparse.co.occurrence.chunked.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/d3plus-network/d3plus.network.R")#
source("/Users/pierre-alex/Dropbox/1-asg/1-production/2-code/keep.top.links.R")#
#
library(data.table)#
library (EconGeo)#
library (jsonlite)#
#
# LOAD HF MODEL LIKE HISTORY#
setwd("~/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/testing")#
links = fread("likes_2024.csv")[, c("username", "id")]#
#
links2 = links#
links2$sum = 1#
links2$sum = ave(links2$sum, links2$id, FUN = sum)#
#links2 = unique (subset(links2[, c("id", "sum")], links2$sum>600))#
links2 = unique (links2[, c("id", "sum")])#
links2 = links2[order(-links2$sum), ]#
n=70#
links2 = links2[1:n, ]#
links = subset (links, links$id %in% links2$id)#
#
nodes = fread("hf_models.csv")#
nodes = subset (nodes, nodes$id %in% links$id)#
parent = fread("hf_models_tasks_parent.csv") # add parents for pipeline tags #
parent$pipeline_tag = gsub(" ", "-", tolower(parent$task))#
nodes = merge (nodes, parent, by = "pipeline_tag", all.x = T)#
nodes$id=nodes$id#
nodes$size = nodes$likes#
nodes$parent = nodes$tag#
nodes$color = nodes$color#
nodes$parent2 = nodes$author#
nodes$likes = nodes$likes#
#
nodes = unique (nodes[, c("id", "size", "parent", "color", #
                          "parent2", "likes")])#
#
#nodes$id <- sub("^[^/]+/", "", nodes$id)#
#links$id <- sub("^[^/]+/", "", links$id)#
#
links = sparse.co.occurrence.chunked(links)#
#
links = relatedness(links, method = "cosine")#
links = get_list(links)#
links = subset (links, links$Count>0)#
colnames (links) = c("from", "to", "weight")#
#
# add most related#
library(dplyr)#
df3 = keep.top.links(links, 5)#
df3 <- df3 %>%#
  group_by(from) %>%#
  summarise(#
    related = paste(#
    sprintf("%s (%0.4f)", #
            to, #
             weight),#
      collapse = ", "#
    )#
  )#
nodes = merge (nodes, df3, by.x = "id", by.y = "from")#
#
dir = "/Users/pierre-alex/Dropbox/PABalland.github.io/hf/api/model/model"#
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/HUGGING-FACE/viz")#
p5 <- paste(readLines("graph-part-5.html"), collapse="\n") #before json data#
topedges2 = 3#
savename = "network-small"#
spread_factor <- 200#
d3plus.network(links)
