reg.level = 2 # nuts2; or 0 (country), nuts1, nuts3
###
### 1. LOAD PATNB-CPC or PATNB-PRIO or PATNB-WIPO
###
# read cpc-prio from regpat
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/PATENTS/REGPAT")
patcpc = fread(paste0("patnb-cpc-", office, ".csv"))
patwipo = fread(paste0("patnb-wipo-", office, ".csv"))
tech.level = c("prio") #, "wipo", "4-digit","3-digit","2-digit","1-digit")
#for (tl in tech.level){
tl = "prio"
if (tl == "prio") {
pat = prio
colnames (pat) = c("patnb", "tech")
priotech = unique (pat$tech)
pat2 = patcpc # add 4 digit for reldens
colnames (pat2) = c("patnb", "tech")
pat2$tech = substr(pat2$tech, 1, 4)
pat2 = unique (pat2)
nonpriotech = unique (pat2$tech)
pat = rbind (pat, pat2)
#dir.create(file.path(save.dir, tl))
#setwd(file.path(save.dir, tl))
}
if (tl == "wipo") {
pat = patwipo
colnames (pat) = c("patnb", "tech")
#dir.create(file.path(save.dir, tl))
#setwd(file.path(save.dir, tl))
}
if (tl == "4-digit") {
pat = patcpc
colnames (pat) = c("patnb", "tech")
pat$tech = substr(pat$tech, 1, 4)
pat = unique(pat)
#dir.create(file.path(save.dir, tl))
#setwd(file.path(save.dir, tl))
}
if (tl == "3-digit") {
pat = patcpc
colnames (pat) = c("patnb", "tech")
pat$tech = substr(pat$tech, 1, 3)
pat = unique(pat)
#dir.create(file.path(save.dir, tl))
#setwd(file.path(save.dir, tl))
}
if (tl == "2-digit") {
pat = patcpc
colnames (pat) = c("patnb", "tech")
pat$tech = substr(pat$tech, 1, 2)
pat = unique(pat)
#dir.create(file.path(save.dir, tl))
#setwd(file.path(save.dir, tl))
}
if (tl == "1-digit") {
pat = patcpc
colnames (pat) = c("patnb", "tech")
pat$tech = substr(pat$tech, 1, 1)
pat = unique(pat)
#dir.create(file.path(save.dir, tl))
#setwd(file.path(save.dir, tl))
}
colnames (pat) = c("patnb", "tech")
# select time frame
pat <- pat[as.integer(substr(pat$patnb, 3, 6)) >= start &
as.integer(substr(pat$patnb, 3, 6)) <= start + period.length, ]
###
### 2. SELECTION GEO UNIVERSE AND REGIONAL LEVEL
###
# dependency: "patnb-inv-pct.csv" or "patnb-inv-epo.csv" [REGPAT] & pat [R]
# return reg.tech (patnb; reg; tech)
# keep pat.inv ()
# read inventor information
pat.inv = fread(paste0("/Users/pierre-alex/Dropbox/1-asg/1-production/1-data/PATENTS/REGPAT/patnb-inv-reg-", office, ".csv"))
# select time frame
pat.inv <- pat.inv[as.integer(substr(pat.inv$pat_nbr, 3, 6)) >= 2018 & #2018/22
as.integer(substr(pat.inv$pat_nbr, 3, 6)) <= 2024, ]
#pat.inv$country = substr(pat.inv$reg_code, 1, 2)
#pat.inv = pat.inv[!grepl("ZZZ$", pat.inv$reg_code), ] # remove unidentified regions
pat.inv = subset (pat.inv, substr(pat.inv$reg_code, 1, 2) %in% ctry) # select countries
colnames (pat.inv) = c("patnb", "reg", "inv")
# AGGREGATE REGCODES INTO NUTS0, 1, 2, 3
if (is.numeric(reg.level) == TRUE) {
pat.inv$reg = substr(pat.inv$reg, 1, reg.level + 2)
}
# AGGREGATE REGCODES INTO FUAS
# AGGREGATE REGCODES INTO CUSTOMIZED REGIONS
# (same idea as with tech prio)
# create patnb-reg-tech
reg.tech = unique(pat.inv[, c("patnb", "reg")])
reg.tech = merge (reg.tech, pat, by = "patnb") # merge with pat
###
### 4. COMPUTE RELATEDNESS BETWEEN TECHNOLOGIES
###
# dependency: prio [R] or reg.tech [R]
# return rel (from; to; value)
if (rel.universe == "all") {
# use all patents for a given office at a given period (prio)
rel = unique(pat[, c("patnb", "tech")])
}
if (rel.universe == "regions.analyzed") {
# or only patents from the geo universe of the study (reg.tech)
rel = unique(reg.tech[, c("patnb", "tech")])
}
rel = fast.co.occurrence (rel) # if nrow < 20 use co_occurrence
rel = get_list(relatedness(rel, method = "cosine")) # could change to other methods
colnames(rel) = c("from", "to", "value")
rel$value = round (rel$value, 3)
rel = rel[order(-rel$value), ]
#setwd(file.path(save.dir, tl))
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/2-analysis/patents")
write.csv (rel, "rel", row.names = F)
###
### 5. COMPUTE COUNTS, RELDENSITY AND COMPLEXITY (NEED 4-DIGIT)
###
### 5.1 COUNT NB OF PATENTS PER REG-TECH
reg.tech$count <- ave(rep(1, nrow(reg.tech)), paste0(reg.tech$tech, reg.tech$reg), FUN = sum)
reg.tech = reg.tech[, c("reg", "tech", "count")] %>% distinct()
reg.tech = get_matrix (reg.tech) # we need the 0s to later compute the reldens when no patents
reg.tech = get_list (reg.tech)
colnames (reg.tech) = c("reg", "tech", "count")
### 5.2 RCA
rca = get_list(rca(get_matrix (reg.tech)))
colnames (rca) = c("reg", "tech", "rca")
reg.tech = merge (reg.tech, rca, by = c("reg", "tech"))
### 5.3 RELATEDNESS DENSITY
rcab = rca(get_matrix(reg.tech[, c("reg", "tech", "count")]), binary = T)
reldens = relatedness_density (rcab, get_matrix(rel))
reldens[is.nan(reldens)] <- 0
reldens = get_list (reldens)
colnames (reldens) = c("reg", "tech", "reldens")
reg.tech = merge (reg.tech, reldens, by = c("reg", "tech"))
### 5.4 COMPLEXITY
comp = get_matrix(reg.tech[, c("reg", "tech", "count")]) # sometimes better from larger universe
comp = rca(comp, binary = T)
comp = data.frame (
tech = colnames (comp),
ub = ubiquity (comp),
mor = round (rescale(mort (comp)),2),
comp = round (rescale(tci (comp)),2) # sometimes need to * -1
)
comp = comp[, c("tech", "comp")]
reg.tech = merge (reg.tech, comp, by = "tech")
reg.tech = reg.tech[, c("reg", "tech", "count", "rca", "reldens", "comp")]
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/2-analysis/patents")
write.csv (reg.tech, "count-rca-reldens-comp.csv", row.names = F)
View(rel)
View(reg.tech)
# packages & codes
library(jsonlite)
library(dplyr)
library(tidyverse)
# query on crunchbase march 20
# Industries: all 7 AI
# Exit: is.blank = T
# Ile de France: 718/32813
# Kyiv: 60/3032
# Moscow: 139/5733
# Istanbul: 148/4839
# London: 2133/67622
# Milan: 104/5349
# Naples: 18/690
# Rome: 73/3251
# Madrid: 220/12394
# Athens: 45/3070
# St Petersburg city: 15/956
# Dortmund	Essen		Duisburg	Bochum	Gelsenkirchen	Oberhausen Hagen	Hamm	M端lheim an der Ruhr	Herne
# 23/3135
# Barcelona: 227/10529
# berlin: 438/13233
# rott/the hague : Zuid Holland 141/11190
# manchester: 57/4954
# hamburg: 121/6620
# birmingham 26/2931
# lisbon: 70/2193
# budapest: 51/2857
# katowice: 7/256
# warsw: 110/2403
# bucharest: 56/3644
# stockholms lan: 200/11453
# munich /munchen : 229/6861
# vienna/Wien: 99/2925
# stuttgart 27/1998
# amsterdam 232/8302
# frankfurt 49/2779
# brussels + brussels hoof... 69/4074
# read
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/3-outputs/ukraine")
df = read.csv ("ai-eu-cities.csv")
View(df)
# packages & codes
library(jsonlite)
library(dplyr)
library(tidyverse)
# query on crunchbase march 20
# Industries: all 7 AI
# Exit: is.blank = T
# Ile de France: 718/32813
# Kyiv: 60/3032
# Moscow: 139/5733
# Istanbul: 148/4839
# London: 2133/67622
# Milan: 104/5349
# Naples: 18/690
# Rome: 73/3251
# Madrid: 220/12394
# Athens: 45/3070
# St Petersburg city: 15/956
# Dortmund	Essen		Duisburg	Bochum	Gelsenkirchen	Oberhausen Hagen	Hamm	M端lheim an der Ruhr	Herne
# 23/3135
# Barcelona: 227/10529
# berlin: 438/13233
# rott/the hague : Zuid Holland 141/11190
# manchester: 57/4954
# hamburg: 121/6620
# birmingham 26/2931
# lisbon: 70/2193
# budapest: 51/2857
# katowice: 7/256
# warsw: 110/2403
# bucharest: 56/3644
# stockholms lan: 200/11453
# munich /munchen : 229/6861
# vienna/Wien: 99/2925
# stuttgart 27/1998
# amsterdam 232/8302
# frankfurt 49/2779
# brussels + brussels hoof... 69/4074
# read
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/3-outputs/ukraine")
df = read.csv ("ai-eu-cities.csv")
df$color[df$prio=="Istanbul"]="#8cab79"
df$color[df$prio=="Moscow"]="#8cab79"
df$color[df$prio=="Kyiv"]="#8cab79"
df$color[df$prio=="Saint Petersburg"]="#8cab79"
ex <- toJSON(df)
setwd("/Users/pierre-alex/Dropbox/PABalland.github.io/ceps/unido/json")
write(ex, "ai-eu-cities.json")
library (EconGeo)
library(Hmisc)
### READ TEST DATA
setwd("~/Dropbox/1-asg/1-production/2-code/d3plus-network")
nodes = read.csv("nodes.csv")
links = read.csv("links.csv")
df$color[df$prio=="Istanbul"]="#8cab79"
df$parent
# packages & codes
library(jsonlite)
library(dplyr)
library(tidyverse)
# query on crunchbase march 20
# Industries: all 7 AI
# Exit: is.blank = T
# Ile de France: 718/32813
# Kyiv: 60/3032
# Moscow: 139/5733
# Istanbul: 148/4839
# London: 2133/67622
# Milan: 104/5349
# Naples: 18/690
# Rome: 73/3251
# Madrid: 220/12394
# Athens: 45/3070
# St Petersburg city: 15/956
# Dortmund	Essen		Duisburg	Bochum	Gelsenkirchen	Oberhausen Hagen	Hamm	M端lheim an der Ruhr	Herne
# 23/3135
# Barcelona: 227/10529
# berlin: 438/13233
# rott/the hague : Zuid Holland 141/11190
# manchester: 57/4954
# hamburg: 121/6620
# birmingham 26/2931
# lisbon: 70/2193
# budapest: 51/2857
# katowice: 7/256
# warsw: 110/2403
# bucharest: 56/3644
# stockholms lan: 200/11453
# munich /munchen : 229/6861
# vienna/Wien: 99/2925
# stuttgart 27/1998
# amsterdam 232/8302
# frankfurt 49/2779
# brussels + brussels hoof... 69/4074
# read
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/3-outputs/ukraine")
df = read.csv ("ai-eu-cities.csv")
df$color[df$prio=="Istanbul"]="#8cab79"
df$color[df$prio=="Moscow"]="#8cab79"
df$color[df$prio=="Kyiv"]="#8cab79"
df$color[df$prio=="Saint Petersburg"]="#8cab79"
df = subset (df, df$parent=="Artificial Intelligence")
df$parent = "EU27 + EFTA"
df$parent[df$prio=="Istanbul"]="Non EU"
df$parent[df$prio=="Moscow"]="Non EU"
df$parent[df$prio=="Kyiv"]="Non EU"
df$parent[df$prio=="Saint Petersburg"]="Non EU"
ex <- toJSON(df)
setwd("/Users/pierre-alex/Dropbox/PABalland.github.io/ceps/unido/json")
write(ex, "ai-cities-crunchbase.json")
View(nodes)
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/3-projects/3-unido/2-analysis/patents")
links = read.csv(rel)
links = read.csv("rel.csv")
rel
#setwd(file.path(save.dir, tl))
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/2-analysis/patents")
write.csv (rel, "rel.csv", row.names = F)
links = read.csv("rel.csv")
View(links)
links = subset (links, substr(links$from)>4)
links = subset (links, nchar(links$from>4))
links = subset (links, nchar(links$from)>4)
View(links)
links = subset (links, nchar(links$to)>4)
View(links)
# extract x and y
graph <- graph_from_data_frame(df, directed = FALSE)
library(igraph)
# extract x and y
graph <- graph_from_data_frame(df, directed = FALSE)
# Calculate the layout (positions for each node)
# Here we use the layout with Fruchterman-Reingold algorithm, which is good for general purposes
layout <- layout_with_fr(graph)
# The layout is a matrix where each row is a node and columns are x and y coordinates
# You can print it to see
print(layout)
# Optionally, you can plot the graph to visualize the layout
plot(graph, layout = layout)
# If you want to store the coordinates in a dataframe
coordinates_df <- as.data.frame(layout)
colnames(coordinates_df) <- c("x", "y")
# Assign names to the rows to match them with node names
row.names(coordinates_df) <- V(graph)$name
print(coordinates_df)
# extract x and y
graph <- graph_from_data_frame(links, directed = FALSE)
# Calculate the layout (positions for each node)
# Here we use the layout with Fruchterman-Reingold algorithm, which is good for general purposes
layout <- layout_with_fr(graph)
# The layout is a matrix where each row is a node and columns are x and y coordinates
# You can print it to see
print(layout)
# Optionally, you can plot the graph to visualize the layout
plot(graph, layout = layout)
# If you want to store the coordinates in a dataframe
coordinates_df <- as.data.frame(layout)
colnames(coordinates_df) <- c("x", "y")
# Assign names to the rows to match them with node names
row.names(coordinates_df) <- V(graph)$name
print(coordinates_df)
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/3-projects/3-unido/2-analysis/patents")
links = read.csv("rel.csv")
links = subset (links, nchar(links$from)>4)
links = subset (links, nchar(links$to)>4)
links=subset (links, links$value>0)
links$weight = links$value
links$value=NULL
top = 5
# keep only top n links per node
links2 = NULL
for (k in unique (links$from)) {
links3 = subset (links, links$from==k)
links3=links3[order(-links3$weight), ]
links3=links3[1:top,]
links2 = rbind (links2, links3)
}
# keep the nodes with 0 links
links3 = subset(links, links$weight==0)
links2 = rbind (links2, links3)
links2 = links2[order(-links2$weight),] # need to sort to then remove 0 links
nodes = nodes[order(match(nodes$id,links2$from)),]
links2=na.omit(links2)
links = links
# extract x and y
graph <- graph_from_data_frame(links, directed = FALSE)
# Calculate the layout (positions for each node)
# Here we use the layout with Fruchterman-Reingold algorithm, which is good for general purposes
layout <- layout_with_fr(graph)
# The layout is a matrix where each row is a node and columns are x and y coordinates
# You can print it to see
print(layout)
# Optionally, you can plot the graph to visualize the layout
plot(graph, layout = layout)
# extract x and y
graph <- graph_from_data_frame(links, directed = T)
# Calculate the layout (positions for each node)
# Here we use the layout with Fruchterman-Reingold algorithm, which is good for general purposes
layout <- layout_with_fr(graph)
# The layout is a matrix where each row is a node and columns are x and y coordinates
# You can print it to see
print(layout)
# Optionally, you can plot the graph to visualize the layout
plot(graph, layout = layout)
library (EconGeo)
library(Hmisc)
library(igraph)
### READ TEST DATA
setwd("~/Dropbox/1-asg/1-production/2-code/d3plus-network")
nodes = read.csv("nodes.csv")
links = read.csv("links.csv")
setwd("/Users/pierre-alex/Dropbox/1-asg/1-production/3-projects/3-unido/2-analysis/patents")
links = read.csv("rel.csv")
links = subset (links, nchar(links$from)>4)
links = subset (links, nchar(links$to)>4)
links=subset (links, links$value>0)
links$weight = links$value
links$value=NULL
top = 5
# keep only top n links per node
links2 = NULL
for (k in unique (links$from)) {
links3 = subset (links, links$from==k)
links3=links3[order(-links3$weight), ]
links3=links3[1:top,]
links2 = rbind (links2, links3)
}
# keep the nodes with 0 links
links3 = subset(links, links$weight==0)
links2 = rbind (links2, links3)
links2 = links2[order(-links2$weight),] # need to sort to then remove 0 links
nodes = nodes[order(match(nodes$id,links2$from)),]
links2=na.omit(links2)
links = links
# extract x and y
graph <- graph_from_data_frame(links, directed = F)
# Calculate the layout (positions for each node)
# Here we use the layout with Fruchterman-Reingold algorithm, which is good for general purposes
layout <- layout_with_fr(graph)
# The layout is a matrix where each row is a node and columns are x and y coordinates
# You can print it to see
print(layout)
# Optionally, you can plot the graph to visualize the layout
plot(graph, layout = layout)
# Barcelona: 227/10529
# berlin: 438/13233
# rott/the hague : Zuid Holland 141/11190
# manchester: 57/4954
# hamburg: 121/6620
# birmingham 26/2931
# lisbon: 70/2193
# budapest: 51/2857
# katowice: 7/256
# warsw: 110/2403
# bucharest: 56/3644
# stockholms lan: 200/11453
# munich /munchen : 229/6861
# vienna/Wien: 99/2925
# stuttgart 27/1998
# amsterdam 232/8302
# frankfurt 49/2779
# brussels + brussels hoof... 69/4074
# read
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/3-outputs/ukraine")
df = read.csv ("ai-eu-cities.csv")
df = subset (df, df$country != "Russia")
df = subset (df, df$country != "Turkey")
View(df)
# packages & codes
library(jsonlite)
library(dplyr)
library(tidyverse)
# query on crunchbase march 20
# Industries: all 7 AI
# Exit: is.blank = T
# Ile de France: 718/32813
# Kyiv: 60/3032
# Moscow: 139/5733
# Istanbul: 148/4839
# London: 2133/67622
# Milan: 104/5349
# Naples: 18/690
# Rome: 73/3251
# Madrid: 220/12394
# Athens: 45/3070
# St Petersburg city: 15/956
# Dortmund	Essen		Duisburg	Bochum	Gelsenkirchen	Oberhausen Hagen	Hamm	M端lheim an der Ruhr	Herne
# 23/3135
# Barcelona: 227/10529
# berlin: 438/13233
# rott/the hague : Zuid Holland 141/11190
# manchester: 57/4954
# hamburg: 121/6620
# birmingham 26/2931
# lisbon: 70/2193
# budapest: 51/2857
# katowice: 7/256
# warsw: 110/2403
# bucharest: 56/3644
# stockholms lan: 200/11453
# munich /munchen : 229/6861
# vienna/Wien: 99/2925
# stuttgart 27/1998
# amsterdam 232/8302
# frankfurt 49/2779
# brussels + brussels hoof... 69/4074
# read
setwd("~/Dropbox/1-asg/1-production/3-projects/3-unido/3-outputs/ukraine")
df = read.csv ("ai-eu-cities.csv")
df = subset (df, df$country != "Russia")
df = subset (df, df$country != "Turkey")
df$color[df$prio=="Kyiv"]="#8cab79"
df = subset (df, df$parent=="Artificial Intelligence")
df$parent = "EU27 + EFTA"
#df$parent[df$prio=="Istanbul"]="Non EU"
#df$parent[df$prio=="Moscow"]="Non EU"
#df$parent[df$prio=="Kyiv"]="Non EU"
#df$parent[df$prio=="Saint Petersburg"]="Non EU"
ex <- toJSON(df)
setwd("/Users/pierre-alex/Dropbox/PABalland.github.io/ceps/unido/json")
write(ex, "ai-cities-crunchbase.json")
View(df)
