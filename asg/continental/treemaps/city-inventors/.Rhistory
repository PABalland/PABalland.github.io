35250/2
evtools::install_github('yihui/tinytex')
devtools::install_github('yihui/tinytex')
devtools::install_github("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
# 1. Plot a network graph
#The first step is to read the list of edges and nodes in this network:
EL = read.csv ("https://raw.githubusercontent.com/PABalland/ON/master/lesmis-el.csv")
head(EL)
NL = read.csv ("https://raw.githubusercontent.com/PABalland/ON/master/lesmis-nl.csv")
head(NL)
#install.packages("igraph")
library (igraph)
g <- graph_from_data_frame(d=EL, vertices=NL, directed=FALSE)
plot (g)
# D3 network (interactive)
EL2 = EL[, 1:2]
head(EL2)
library (networkD3)
simpleNetwork(EL2)
# library(devtools)
# devtools::install_github("PABalland/EconGeo", force = T)
library (EconGeo)
# Import the data (read the csv file)
M = read.csv ("https://paballand.github.io/teaching/ids/msa.sub.cat.pat.count.csv")
# Subset the data by keeping the decade 2000 only
M2000 = subset (M, dec == 2000)
# Keep only the variables "Cbsa.Name", "NBER.Sub.Cat", "pat.count" for this decade
M2000 = M2000[, c("Cbsa.Name", "NBER.Sub.Cat", "pat.count")]
# Transform the data into an adjacency matrix (using "get.matrix")
M2000 = get.matrix (M2000)
# Compute the co-occurrences of technologies (co.occurrence)
c = co.occurrence (t(M2000))
# Compute relatedness between technologies and make the matrix binary
r = relatedness (c)
r
EL = get.list (r)
View(EL)
head(NL)
# plot
g = graph_from_adjacency_matrix(r, weighted=TRUE, mode="undirected", diag=FALSE)
plot (g)
head(M)
# Subset the data by keeping the decade 2000 only
M2000 = subset (M, dec == 2000)
# Keep only the variables "Cbsa.Name", "NBER.Sub.Cat", "pat.count" for this decade
M2000 = M2000[, c("Cbsa.Name", "NBER.Sub.Cat.Name", "pat.count")]
# Transform the data into an adjacency matrix (using "get.matrix")
M2000 = get.matrix (M2000)
# Compute the co-occurrences of technologies (co.occurrence)
c = co.occurrence (t(M2000))
# Compute relatedness between technologies and make the matrix binary
r = relatedness (c)
# plot
g = graph_from_adjacency_matrix(r, weighted=TRUE, mode="undirected", diag=FALSE)
plot (g)
r[r<1] = 0
r[r>1] = 1
g = graph_from_adjacency_matrix(r, weighted=TRUE, mode="undirected", diag=FALSE)
plot (g)
# Compute the binary version of the relative advantage of a
# all cities in all technologies (RCA)
rca = RCA(M2000, binary = T)
# 9. Compute relatedness density
rd = relatedness.density(rca, r)
View(rd)
library('rvest')
#Specifying the url for desired website to be scraped
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
#Reading the HTML code from the website
webpage <- read_html(url)
webpage
#Specifying the url for desired website to be scraped
url <- 'https://deymeaufildesrues.wordpress.com/les-deymois-racontent/les-cathares-ques-aco/'
#Reading the HTML code from the website
read_html(url)
#Reading the HTML code from the website
webpage = read_html(url)
View(webpage)
library(httr)
#GET HTML page and transfer it to tables
rlo <- httr::GET("http://baseballaustria.com/regionalliga-ost/")
rlo
rlo <- xml2::read_html(rlo)
rlo
#GET HTML page and transfer it to tables
rlo <- httr::GET("http://baseballaustria.com/regionalliga-ost/")
rlo <- xml2::read_html(rlo)
#read out "table node"
schedule <- rvest::html_nodes(rlo, css = "table")[2]
#Get the raw schedule
schedule <- rvest::html_table(schedule)
schedule
rlo
#Specifying the url for desired website to be scraped
url <- 'https://deymeaufildesrues.wordpress.com/les-deymois-racontent/les-cathares-ques-aco/'
#Reading the HTML code from the website
webpage = read_html(url)
View(webpage)
url <- 'https://deymeaufildesrues.wordpress.com/les-deymois-racontent/les-cathares-ques-aco/'
#Reading the HTML code from the website
webpage = read_html(url)
title_data_html <- html_nodes(webpage,'entry-content')
View(title_data_html)
View(webpage)
xml_child(webpage, 2)
tolower("G06F - ELECTRIC DIGITAL DATA PROCESSING")
tolower("H04L - TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION")
7.50*0.55+	0.45(6.50+8.00+8.50)/3
7.50*0.55+0.45(6.50+8.00+8.50)/3
7.50*0.55+0.45*(6.50+8.00+8.50)/3
5000 + 500 + 1500
347.60*0.002
16.34*1008.387
16477.04*0.9/12
16477.04*0.9/360
0.025*370
0.01649514*400
17.5*1008.387
17.5*1008.387*0.9/360
17.5*1200*0.9/360
50000*0.2
50000*0.2/12
1000*0.2/12
100000*0.2/12
100*0.9/12
20000-12826.85
22000-12826.85
10029.89/4885
5029.89/2.053202
2449.778*2.053202
814*2
40*12
40*52
40*48
40*46
40*42
1660*0.2
540/145
(828+30+30)/920
(828+30+30)/9.20
(828+30+30)/0.920
(828+30+30)/9.20
(828+30+30)/92.0
(828+30+30)/89
4+8
7+8
11+7
11+8
5+2+3.8
10.8/2
425
#mur a bancher de 27;
tolower("RICHIESTA APERTURA APPARTAMENTO USO TURISTICO “alloggi brevi”")
22-4
1250
1450+
1750+
950+
650+
450+
0+
1500+
1250+
1250+
1200+
850+
400
1250+
1450+
1750+
1325 +
650+
450+
1500+
1250+
1250 +
1200 +
850 +
400
27307/5461
27307/5461.55
6375*0.2
6375*0.8
options(stringsAsFactors=FALSE)
library (jsonlite)
library (EconGeo)
library (RColorBrewer)
library (data.table)
library (dplyr)
dir  = "D:/Dropbox/2-private/1-asg/1-production/2-continental" # for analysis
dir.regpat = "D:/Dropbox/2-private/1-asg/1-production/_DATA/PATENTS/REGPAT" # for latest regpat version
web = "D:/Dropbox/2-private/PABalland.github.io/asg/continental"
# load different parts of html
setwd("D:/Dropbox/2-private/1-asg/1-production/_SOURCE/treemaps")
p1 = paste(readLines("part-1.html"), collapse="\n") #before json data
p3 = paste(readLines("part-3-patents-applicants.html"), collapse="\n") #after json data
setwd("D:/Dropbox/2-private/1-asg/1-production/_SOURCE")
source ("prettyprint.df.R")
# read priorities
setwd(paste0(dir, "/1-data"))
df = fread ("pctnb-prio.csv", sep = ",", header = T)
# read app info
setwd(dir.regpat)
app = fread ("pctnb-app-reg.csv", sep = ",", header = T)
app$ctry = substr(app$reg_code, 0, 2)
app = app %>% distinct ()
df = inner_join(df, app, by = "pct_nbr")
df$year = as.numeric(substr(df$pct_nbr, 3, 6))
df = subset (df, df$year >= 2017) # select period
df$year = NULL
# merge with fua information
setwd(dir.regpat)
reg.att = read.csv2 ("reg-att.csv")[, c("reg_code", "fua_id", "fua_name")] %>% distinct()
df = inner_join(df, reg.att, by = "reg_code") # merge to get priority years
df$reg_code = NULL
df = df %>% distinct()
df <- df[complete.cases(df), ]
df$freq = 1
df$ID = paste0(df$priority, df$app_name, df$fua_name)
df$count = ave(df$freq, df$ID, FUN = sum)
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(df$countcity),]
df = df[, c("priority", "app_name", "fua_id", "fua_name", "count", "ctry")]
df = df %>% distinct ()
df$region = df$fua_name
links = NULL
head(df)
dfs = subset (df, df$priority == "Computer-aided design [CAD]")
View(dfs)
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(-df$countcity),]
df$freq = 1
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(-df$countcity),]
options(stringsAsFactors=FALSE)
library (jsonlite)
library (EconGeo)
library (RColorBrewer)
library (data.table)
library (dplyr)
dir  = "D:/Dropbox/2-private/1-asg/1-production/2-continental" # for analysis
dir.regpat = "D:/Dropbox/2-private/1-asg/1-production/_DATA/PATENTS/REGPAT" # for latest regpat version
web = "D:/Dropbox/2-private/PABalland.github.io/asg/continental"
# load different parts of html
setwd("D:/Dropbox/2-private/1-asg/1-production/_SOURCE/treemaps")
p1 = paste(readLines("part-1.html"), collapse="\n") #before json data
p3 = paste(readLines("part-3-patents-applicants.html"), collapse="\n") #after json data
setwd("D:/Dropbox/2-private/1-asg/1-production/_SOURCE")
source ("prettyprint.df.R")
# read priorities
setwd(paste0(dir, "/1-data"))
df = fread ("pctnb-prio.csv", sep = ",", header = T)
# read app info
setwd(dir.regpat)
app = fread ("pctnb-app-reg.csv", sep = ",", header = T)
app$ctry = substr(app$reg_code, 0, 2)
app = app %>% distinct ()
df = inner_join(df, app, by = "pct_nbr")
df$year = as.numeric(substr(df$pct_nbr, 3, 6))
df = subset (df, df$year >= 2017) # select period
df$year = NULL
# merge with fua information
setwd(dir.regpat)
reg.att = read.csv2 ("reg-att.csv")[, c("reg_code", "fua_id", "fua_name")] %>% distinct()
df = inner_join(df, reg.att, by = "reg_code") # merge to get priority years
df$reg_code = NULL
df = df %>% distinct()
df <- df[complete.cases(df), ]
df$freq = 1
df$ID = paste0(df$priority, df$app_name, df$fua_name)
df$count = ave(df$freq, df$ID, FUN = sum)
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(df$countcity),]
df = df[, c("priority", "app_name", "fua_id", "fua_name", "count", "ctry")]
df = df %>% distinct ()
df$region = df$fua_name
links = NULL
df$freq = 1
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(-df$countcity),]
View(df)
df = df[order(-df$count),]
View(df)
df = df[order(-df$countcity),]
View(df)
head(df)
df = df[, c("priority", "fua_name", "app_name", "count", "ctry")]
View(df)
df = df[, c("priority", "fua_name", "app_name", "count", "ctry")]
setwd(paste0(web, "/treemaps/city-applicants"))
write.csv2(df, "city-applicants.csv", row.names = F)
options(stringsAsFactors=FALSE)
library (jsonlite)
library (EconGeo)
library (RColorBrewer)
library (data.table)
library (dplyr)
dir  = "D:/Dropbox/2-private/1-asg/1-production/2-continental" # for analysis
dir.regpat = "D:/Dropbox/2-private/1-asg/1-production/_DATA/PATENTS/REGPAT" # for latest regpat version
web = "D:/Dropbox/2-private/PABalland.github.io/asg/continental"
# load different parts of html
setwd("D:/Dropbox/2-private/1-asg/1-production/_SOURCE/treemaps")
p1 = paste(readLines("part-1.html"), collapse="\n") #before json data
p3 = paste(readLines("part-3-patents-applicants.html"), collapse="\n") #after json data
setwd("D:/Dropbox/2-private/1-asg/1-production/_SOURCE")
source ("prettyprint.df.R")
# read priorities
setwd(paste0(dir, "/1-data"))
df = fread ("pctnb-prio.csv", sep = ",", header = T)
# read app info
setwd(dir.regpat)
app = fread ("pctnb-inv-reg.csv", sep = ",", header = T)
app$ctry = substr(app$reg_code, 0, 2)
app = app %>% distinct ()
df = inner_join(df, app, by = "pct_nbr")
df$year = as.numeric(substr(df$pct_nbr, 3, 6))
df = subset (df, df$year >= 2017) # select period
df$year = NULL
# merge with fua information
setwd(dir.regpat)
reg.att = read.csv2 ("reg-att.csv")[, c("reg_code", "fua_id", "fua_name")] %>% distinct()
df = inner_join(df, reg.att, by = "reg_code") # merge to get priority years
df$reg_code = NULL
df = df %>% distinct()
df <- df[complete.cases(df), ]
df$freq = 1
df$ID = paste0(df$priority, df$inv_name, df$fua_name)
df$count = ave(df$freq, df$ID, FUN = sum)
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(df$countcity),]
df = df[, c("priority", "inv_name", "fua_id", "fua_name", "count", "ctry")]
df = df %>% distinct ()
df$region = df$fua_name
links = NULL
head(df)
df$freq = 1
df$countcity = ave(df$freq, df$fua_name, FUN = sum)
df = df[order(-df$count),]
df = df[order(-df$countcity),]
df = df[, c("priority", "fua_name", "inv_name", "count", "ctry")]
head(df)
df = df[, c("priority", "fua_name", "inv_name", "count", "ctry")]
setwd(paste0(web, "/treemaps/city-inventors"))
write.csv2(df, "city-inventors.csv", row.names = F)
23891.67-4200
